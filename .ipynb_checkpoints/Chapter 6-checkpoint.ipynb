{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f24dc49",
   "metadata": {},
   "source": [
    "# Previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e548404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy() # x > 0\n",
    "        out[self.mask] = 0 # x <= 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0 # x <= 0\n",
    "        dx = dout # x > 0\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def softmax(x):\n",
    "    C = np.max(x, axis=1).reshape(x.shape[0], 1)\n",
    "    e = np.exp(x - C)\n",
    "    s = np.sum(e, axis=1).reshape(x.shape[0], 1)\n",
    "    \n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aea3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = weight_init_std * np.random.randn(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = weight_init_std * np.random.randn(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617d0e",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae49d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6ea368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nemok\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "dat = mnist.data.to_numpy()\n",
    "tar = mnist.target.to_numpy()\n",
    "\n",
    "x_train = dat[:60000,]\n",
    "x_test = dat[60000:,]\n",
    "t_train = tar[:60000]\n",
    "t_test = tar[60000:]\n",
    "\n",
    "I = np.eye(10)\n",
    "t_train = I[t_train.astype(int)]\n",
    "t_test = I[t_test.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607a916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.17291666666666666 | 0.1753\n",
      "train acc, test acc : 0.7432 | 0.7499\n",
      "train acc, test acc : 0.82205 | 0.8297\n",
      "train acc, test acc : 0.8356833333333333 | 0.8431\n",
      "train acc, test acc : 0.8358 | 0.8423\n",
      "train acc, test acc : 0.8390666666666666 | 0.8473\n",
      "train acc, test acc : 0.8455166666666667 | 0.8539\n",
      "train acc, test acc : 0.8560833333333333 | 0.865\n",
      "train acc, test acc : 0.8578666666666667 | 0.8671\n",
      "train acc, test acc : 0.87075 | 0.8782\n",
      "train acc, test acc : 0.8794 | 0.8841\n",
      "train acc, test acc : 0.8813666666666666 | 0.8881\n",
      "train acc, test acc : 0.8897833333333334 | 0.8942\n",
      "train acc, test acc : 0.88995 | 0.8942\n",
      "train acc, test acc : 0.9001833333333333 | 0.9036\n",
      "train acc, test acc : 0.8955666666666666 | 0.8975\n",
      "train acc, test acc : 0.9059 | 0.9083\n",
      "train acc, test acc : 0.9013333333333333 | 0.9042\n",
      "train acc, test acc : 0.9146 | 0.9164\n",
      "train acc, test acc : 0.9163166666666667 | 0.9178\n",
      "train acc, test acc : 0.9228166666666666 | 0.9222\n",
      "train acc, test acc : 0.9213833333333333 | 0.9219\n",
      "train acc, test acc : 0.9290666666666667 | 0.9295\n",
      "train acc, test acc : 0.9255666666666666 | 0.9255\n",
      "train acc, test acc : 0.9306166666666666 | 0.932\n",
      "train acc, test acc : 0.9361666666666667 | 0.9361\n",
      "train acc, test acc : 0.9364833333333333 | 0.936\n",
      "train acc, test acc : 0.9355833333333333 | 0.9345\n",
      "train acc, test acc : 0.9386333333333333 | 0.9392\n",
      "train acc, test acc : 0.94235 | 0.9435\n",
      "train acc, test acc : 0.9385333333333333 | 0.9399\n",
      "train acc, test acc : 0.94315 | 0.9425\n",
      "train acc, test acc : 0.9435166666666667 | 0.9436\n",
      "train acc, test acc : 0.9449666666666666 | 0.9453\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "optimizer = SGD()\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = 300\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    x_batch = (x_batch - x_batch.mean()) / x_batch.std()\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    params = network.params\n",
    "    optimizer.update(params, grad)\n",
    "    \n",
    "    #for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    #    network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #loss = network.loss(x_batch, t_batch)\n",
    "    #train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \" | \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3211aa6",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c27190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            \n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a23bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.12826666666666667 | 0.1318\n",
      "train acc, test acc : 0.86565 | 0.8723\n",
      "train acc, test acc : 0.9063166666666667 | 0.9096\n",
      "train acc, test acc : 0.9356333333333333 | 0.9388\n",
      "train acc, test acc : 0.9375666666666667 | 0.9373\n",
      "train acc, test acc : 0.9449166666666666 | 0.9438\n",
      "train acc, test acc : 0.94175 | 0.9427\n",
      "train acc, test acc : 0.9559166666666666 | 0.9517\n",
      "train acc, test acc : 0.95435 | 0.9523\n",
      "train acc, test acc : 0.95275 | 0.9471\n",
      "train acc, test acc : 0.94395 | 0.9379\n",
      "train acc, test acc : 0.9570666666666666 | 0.9529\n",
      "train acc, test acc : 0.9643166666666667 | 0.9562\n",
      "train acc, test acc : 0.9562 | 0.9477\n",
      "train acc, test acc : 0.9616 | 0.9561\n",
      "train acc, test acc : 0.9536666666666667 | 0.9468\n",
      "train acc, test acc : 0.9633833333333334 | 0.9539\n",
      "train acc, test acc : 0.9647333333333333 | 0.9536\n",
      "train acc, test acc : 0.9568166666666666 | 0.9484\n",
      "train acc, test acc : 0.9548166666666666 | 0.9453\n",
      "train acc, test acc : 0.9655833333333333 | 0.9569\n",
      "train acc, test acc : 0.96485 | 0.9555\n",
      "train acc, test acc : 0.9524166666666667 | 0.9402\n",
      "train acc, test acc : 0.9569833333333333 | 0.944\n",
      "train acc, test acc : 0.9526 | 0.9412\n",
      "train acc, test acc : 0.9637333333333333 | 0.9526\n",
      "train acc, test acc : 0.9663666666666667 | 0.9525\n",
      "train acc, test acc : 0.9611333333333333 | 0.9508\n",
      "train acc, test acc : 0.9687666666666667 | 0.9583\n",
      "train acc, test acc : 0.9692833333333334 | 0.9595\n",
      "train acc, test acc : 0.9604833333333334 | 0.9493\n",
      "train acc, test acc : 0.9688333333333333 | 0.9566\n",
      "train acc, test acc : 0.9663166666666667 | 0.9556\n",
      "train acc, test acc : 0.97425 | 0.9625\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "optimizer = Momentum()\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = 300\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    x_batch = (x_batch - x_batch.mean()) / x_batch.std()\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    params = network.params\n",
    "    optimizer.update(params, grad)\n",
    "    \n",
    "    #for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    #    network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #loss = network.loss(x_batch, t_batch)\n",
    "    #train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \" | \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579b8ae",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7617ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1421d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc : 0.24983333333333332 | 0.2506\n",
      "train acc, test acc : 0.8684333333333333 | 0.875\n",
      "train acc, test acc : 0.8875833333333333 | 0.8936\n",
      "train acc, test acc : 0.9081833333333333 | 0.9084\n",
      "train acc, test acc : 0.9075166666666666 | 0.9111\n",
      "train acc, test acc : 0.9115333333333333 | 0.9166\n",
      "train acc, test acc : 0.9204333333333333 | 0.9225\n",
      "train acc, test acc : 0.9243666666666667 | 0.9254\n",
      "train acc, test acc : 0.93125 | 0.9311\n",
      "train acc, test acc : 0.9315 | 0.9324\n",
      "train acc, test acc : 0.9333666666666667 | 0.9329\n",
      "train acc, test acc : 0.9385666666666667 | 0.9377\n",
      "train acc, test acc : 0.9407666666666666 | 0.9415\n",
      "train acc, test acc : 0.9398833333333333 | 0.9413\n",
      "train acc, test acc : 0.9402666666666667 | 0.9406\n",
      "train acc, test acc : 0.9431333333333334 | 0.943\n",
      "train acc, test acc : 0.94435 | 0.9444\n",
      "train acc, test acc : 0.9450333333333333 | 0.9453\n",
      "train acc, test acc : 0.94365 | 0.9429\n",
      "train acc, test acc : 0.9447333333333333 | 0.9432\n",
      "train acc, test acc : 0.9443 | 0.9437\n",
      "train acc, test acc : 0.9447666666666666 | 0.9426\n",
      "train acc, test acc : 0.94775 | 0.9453\n",
      "train acc, test acc : 0.9485166666666667 | 0.9463\n",
      "train acc, test acc : 0.9485333333333333 | 0.9461\n",
      "train acc, test acc : 0.9483 | 0.9455\n",
      "train acc, test acc : 0.94925 | 0.9465\n",
      "train acc, test acc : 0.9507666666666666 | 0.9473\n",
      "train acc, test acc : 0.9514 | 0.9494\n",
      "train acc, test acc : 0.9512166666666667 | 0.9478\n",
      "train acc, test acc : 0.9525833333333333 | 0.9484\n",
      "train acc, test acc : 0.95265 | 0.9489\n",
      "train acc, test acc : 0.9513 | 0.947\n",
      "train acc, test acc : 0.951 | 0.9464\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "optimizer = AdaGrad()\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = 300\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    x_batch = (x_batch - x_batch.mean()) / x_batch.std()\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    params = network.params\n",
    "    optimizer.update(params, grad)\n",
    "    \n",
    "    #for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    #    network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #loss = network.loss(x_batch, t_batch)\n",
    "    #train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \" | \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea3e736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
